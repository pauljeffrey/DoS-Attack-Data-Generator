{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Packet Data Generation.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1gD_LnUIa9s057YIjxzGM7zV7y1ofTj0z","authorship_tag":"ABX9TyN9gAUwuZqbhd6p/0QCgzx/"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Introduction\n","I develop a custom model to generate network packet data. The full architecture of the model was designed by me. Model has an encoder that encodes the columns and generates packet data given any specific column. It is fully developed in tensorflow and all preprocessing steps are included in this file. The dataset has over 100k samples and 32 columns.\n","\n","After training, the model was used to generate over 300,000 unique network packet datapoints with minimal error!"],"metadata":{"id":"QKkGTh5L4BF9"}},{"cell_type":"code","metadata":{"id":"xTb2pGbCxK-t"},"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import os\n","from tensorflow.keras.layers.experimental import preprocessing\n","import time\n","from keras.layers import Dropout\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X_4SpRIpg4bd"},"source":["## Introduction\n","The below code changes my working directory to the initialized FILE_PATH. It's a path to a created directory on my google drive where I have stored all the resources and model I need for the code in this notebook to function. To make this work for you, simply create a directory, upload the 'output.csv' csv file into it and edit the FILE_PATH variable to the directory path (of the directory you just created).\n","\n","All the code have been carefully written and documented in this notebook. "]},{"cell_type":"code","metadata":{"id":"w66MxkrXyEkL"},"source":["FILE_PATH = '/content/drive/MyDrive/Deep Learning/Dataset generation'\n","os.chdir(FILE_PATH)\n","\n","packet_data = pd.read_csv('output.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":350},"id":"KPoqqKMmygXR","executionInfo":{"status":"ok","timestamp":1632610842878,"user_tz":-60,"elapsed":7,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"2b4e3e67-7171-49d5-d822-d9da503022ae"},"source":["packet_data.head(10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ip.id</th>\n","      <th>ip.flags</th>\n","      <th>ip.flags.rb</th>\n","      <th>_ws.col.Protocol</th>\n","      <th>ip.flags.df</th>\n","      <th>ip.flags.mf</th>\n","      <th>ip.frag_offset</th>\n","      <th>ip.ttl</th>\n","      <th>ip.proto</th>\n","      <th>ip.checksum</th>\n","      <th>ip.src</th>\n","      <th>ip.dst</th>\n","      <th>ip.len</th>\n","      <th>ip.dsfield</th>\n","      <th>tcp.srcport</th>\n","      <th>tcp.dstport</th>\n","      <th>tcp.seq</th>\n","      <th>tcp.ack</th>\n","      <th>tcp.len</th>\n","      <th>tcp.hdr_len</th>\n","      <th>tcp.flags</th>\n","      <th>tcp.flags.fin</th>\n","      <th>tcp.flags.syn</th>\n","      <th>tcp.flags.reset</th>\n","      <th>tcp.flags.push</th>\n","      <th>tcp.flags.ack</th>\n","      <th>tcp.flags.urg</th>\n","      <th>tcp.flags.cwr</th>\n","      <th>tcp.window_size</th>\n","      <th>tcp.checksum</th>\n","      <th>tcp.urgent_pointer</th>\n","      <th>tcp.options.mss_val</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0x00009314</td>\n","      <td>0x00004000</td>\n","      <td>0.0</td>\n","      <td>MPEG PES</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>64.0</td>\n","      <td>17.0</td>\n","      <td>0x0000d5d2</td>\n","      <td>10.0.0.23</td>\n","      <td>192.168.2.7</td>\n","      <td>1344.0</td>\n","      <td>0x00000000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0x000047d3</td>\n","      <td>0x00004000</td>\n","      <td>0.0</td>\n","      <td>TCP</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>64.0</td>\n","      <td>6.0</td>\n","      <td>0x0000272a</td>\n","      <td>10.0.0.16</td>\n","      <td>192.168.1.7</td>\n","      <td>60.0</td>\n","      <td>0x00000000</td>\n","      <td>32968.0</td>\n","      <td>1883.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>40.0</td>\n","      <td>0x00000002</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>29200.0</td>\n","      <td>0x000073e5</td>\n","      <td>0.0</td>\n","      <td>1460.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0x0000bb2e</td>\n","      <td>0x00004000</td>\n","      <td>0.0</td>\n","      <td>TCP</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>64.0</td>\n","      <td>6.0</td>\n","      <td>0x0000b3d4</td>\n","      <td>10.0.0.10</td>\n","      <td>192.168.1.7</td>\n","      <td>60.0</td>\n","      <td>0x00000000</td>\n","      <td>42990.0</td>\n","      <td>1883.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>40.0</td>\n","      <td>0x00000002</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>29200.0</td>\n","      <td>0x0000d261</td>\n","      <td>0.0</td>\n","      <td>1460.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0x00000000</td>\n","      <td>0x00004000</td>\n","      <td>0.0</td>\n","      <td>TCP</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>63.0</td>\n","      <td>6.0</td>\n","      <td>0x00006ffd</td>\n","      <td>192.168.1.7</td>\n","      <td>10.0.0.16</td>\n","      <td>60.0</td>\n","      <td>0x00000000</td>\n","      <td>1883.0</td>\n","      <td>32968.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>40.0</td>\n","      <td>0x00000012</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>28960.0</td>\n","      <td>0x0000f36b</td>\n","      <td>0.0</td>\n","      <td>1460.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0x00000000</td>\n","      <td>0x00004000</td>\n","      <td>0.0</td>\n","      <td>TCP</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>63.0</td>\n","      <td>6.0</td>\n","      <td>0x00007003</td>\n","      <td>192.168.1.7</td>\n","      <td>10.0.0.10</td>\n","      <td>60.0</td>\n","      <td>0x00000000</td>\n","      <td>1883.0</td>\n","      <td>42990.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>40.0</td>\n","      <td>0x00000012</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>28960.0</td>\n","      <td>0x00002858</td>\n","      <td>0.0</td>\n","      <td>1460.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0x0000bb2f</td>\n","      <td>0x00004000</td>\n","      <td>0.0</td>\n","      <td>TCP</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>64.0</td>\n","      <td>6.0</td>\n","      <td>0x0000b3db</td>\n","      <td>10.0.0.10</td>\n","      <td>192.168.1.7</td>\n","      <td>52.0</td>\n","      <td>0x00000000</td>\n","      <td>42990.0</td>\n","      <td>1883.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>32.0</td>\n","      <td>0x00000010</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>29200.0</td>\n","      <td>0x0000c120</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0x000047d4</td>\n","      <td>0x00004000</td>\n","      <td>0.0</td>\n","      <td>TCP</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>64.0</td>\n","      <td>6.0</td>\n","      <td>0x00002731</td>\n","      <td>10.0.0.16</td>\n","      <td>192.168.1.7</td>\n","      <td>52.0</td>\n","      <td>0x00000000</td>\n","      <td>32968.0</td>\n","      <td>1883.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>32.0</td>\n","      <td>0x00000010</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>29200.0</td>\n","      <td>0x00008c33</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0x000047d5</td>\n","      <td>0x00004000</td>\n","      <td>0.0</td>\n","      <td>MQTT</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>64.0</td>\n","      <td>6.0</td>\n","      <td>0x00002700</td>\n","      <td>10.0.0.16</td>\n","      <td>192.168.1.7</td>\n","      <td>100.0</td>\n","      <td>0x00000000</td>\n","      <td>32968.0</td>\n","      <td>1883.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>48.0</td>\n","      <td>32.0</td>\n","      <td>0x00000018</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>29200.0</td>\n","      <td>0x00001194</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0x0000bb30</td>\n","      <td>0x00004000</td>\n","      <td>0.0</td>\n","      <td>MQTT</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>64.0</td>\n","      <td>6.0</td>\n","      <td>0x0000b3aa</td>\n","      <td>10.0.0.10</td>\n","      <td>192.168.1.7</td>\n","      <td>100.0</td>\n","      <td>0x00000000</td>\n","      <td>42990.0</td>\n","      <td>1883.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>48.0</td>\n","      <td>32.0</td>\n","      <td>0x00000018</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>29200.0</td>\n","      <td>0x00004284</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0x00005cc5</td>\n","      <td>0x00004000</td>\n","      <td>0.0</td>\n","      <td>TCP</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>63.0</td>\n","      <td>6.0</td>\n","      <td>0x00001340</td>\n","      <td>192.168.1.7</td>\n","      <td>10.0.0.16</td>\n","      <td>52.0</td>\n","      <td>0x00000000</td>\n","      <td>1883.0</td>\n","      <td>32968.0</td>\n","      <td>1.0</td>\n","      <td>49.0</td>\n","      <td>0.0</td>\n","      <td>32.0</td>\n","      <td>0x00000010</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>28960.0</td>\n","      <td>0x00008c12</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        ip.id    ip.flags  ...  tcp.urgent_pointer tcp.options.mss_val\n","0  0x00009314  0x00004000  ...                 NaN                 NaN\n","1  0x000047d3  0x00004000  ...                 0.0              1460.0\n","2  0x0000bb2e  0x00004000  ...                 0.0              1460.0\n","3  0x00000000  0x00004000  ...                 0.0              1460.0\n","4  0x00000000  0x00004000  ...                 0.0              1460.0\n","5  0x0000bb2f  0x00004000  ...                 0.0                 NaN\n","6  0x000047d4  0x00004000  ...                 0.0                 NaN\n","7  0x000047d5  0x00004000  ...                 0.0                 NaN\n","8  0x0000bb30  0x00004000  ...                 0.0                 NaN\n","9  0x00005cc5  0x00004000  ...                 0.0                 NaN\n","\n","[10 rows x 32 columns]"]},"metadata":{},"execution_count":88}]},{"cell_type":"markdown","metadata":{"id":"RJTszzx3hPIu"},"source":["Examining the data, there are 32 columns and about 112373 samples in the dataset as we can see below."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1UV9GuzV0IGG","executionInfo":{"status":"ok","timestamp":1632610846478,"user_tz":-60,"elapsed":451,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"7d080ade-9902-4acd-f88c-952ff68fe3ec"},"source":["len(packet_data)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["112373"]},"metadata":{},"execution_count":89}]},{"cell_type":"markdown","metadata":{"id":"3jmgk-Gnm689"},"source":["## Model Architecture\n","The model makes use of the LSTM model as its core generative system. The model works by generating a character at every time step.\n","\n","Based on the nature of the dataset (32 columns), there are basically 2 ways to model the data:\n","\n","- A basic (vanilla) generator system.\n","- An encoder-based generator system.\n","\n","In both cases, the systems attempts to learn and model the probability distribution responsible for the production of the real samples in the dataset.\n","\n","### Basic generator system\n","\n","Each sample in the dataset is modelled as one long sequence by concatenating each column values to form one long sequence. During concatenation, a unique separator (e.g ' '  etc) is used to separate each column values. This can be viewed as each column being a word in a full sentence (the result of the full concatenation).\n","\n","This model then predicts the next character in the sequence given the previous character. This is the way the model weights are updated. This architecture will have an:\n","- Embedding system: that converts every character to a vector (digital representation).\n","\n","- LSTM model: It takes the previous embedded character as input and outputs the prediction of the next character at every given time step.\n","\n","- Fully connected layer: It makes the final prediction over all words in the model's vocabulary given the lstm's output.\n","\n","The downside to this architecture is that it introduces a lot of uncertainties and errors during training and testing. This is because the algorithm is forced to model longer sequences of characters, generalize over all columns and their unique characters (learns the probability distribution responsible for all characters which is not dependent on the unique columns). In addition, for every time step (character step), there will always be more variation/possibilities as to the model's output because there is no constraint or condition on the model's output. This is as a result of the fact that each column has a set of characters that it can contain. For example some columns are plain numbers (e.g 1.0), while others have letters mixed with numbers (e.g 0x00000014) or just capital letters (e.g TCP). Therefore, the model does not attend to the categories (particular kind of character) during its generation and the error increases this way.\n","\n","### Encoder-based generator system\n","\n","This model is made up of :\n","- An encoder: The columns are represented using one-hot encoding (using a one-hot encoder) and then passed through a 2-layered neural network to get a unique representation (logit values: linear activation values only).\n","\n","- LSTM model: It takes as input both this unique representation of the column and the embedding representation of the previous character.\n","\n","- Fully connected layer: It makes the final prediction over all words in the model's vocabulary given the lstm's output.\n","\n","In this architecture, the encoder helps to constrain the model output by giving the model information about the column its currently trying to generate characters for. This is in turn will put a constraint on the type of characters that the model generates. Consequently, it reduces the loss error of the model as the model learns to generate only relevant characters given or based on the column its generating for at time step t.\n","\n","This is the model used in this notebook. There are 2 ways to train this model and both methods are defined as classes in the next cell below :\n","\n","- PacketGeneratorSlow: It iterates through each column (first loop), encodes the column and then iteratively generates the sequence of characters (second loop) based on the encoded column representation. Since the hidden state (output ) of the lstm model will always be passed as input subsequently at every time step t + 1 and through all the columns, there will be some sort of dependency between each columns. However, training, and data generation will be slower here. Also, optimization might be worse in this case.\n","\n","\n","- PacketGeneratorModel2: It parallelizes the data generation by getting rid of the first iterative step (first loop). It focuses on just generating sequences based on the individual encoded column representations. In this method, the hidden state of the lstm model is not passed through all columns but instead passed as input only during generation of the next character within a column. This method is faster and has no relationship or dependency between the columns.\n","\n","\n","The code for this model follows. We only focus on the second type during training. We use 256 neural units for the embedding layer, 512 units for the last encoder layer, 1024 units for the lstm layer. However, this value can be played with to get best performance.\n","\n","We also define a customized loss class that handles the calculation of the loss error during training."]},{"cell_type":"code","metadata":{"id":"o-DHC1m2m2BT"},"source":["# Define the loss class responsible for calculating the error loss\n","class MaskedLoss(tf.keras.losses.Loss):\n","  def __init__(self):\n","    self.name = 'masked_loss'\n","    # Initialize categorical cross entropy class:\n","    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","        from_logits=True, reduction='none')\n","\n","  def __call__(self, y_true, y_pred):\n","    # Calculate the loss for each item in the batch.\n","    loss = self.loss(y_true, y_pred)\n","\n","    # Mask off the losses on the elements that were padded in the hpe sentences.\n","    mask = tf.cast(y_true != 0, tf.float32)\n","    loss *= mask\n","\n","    # Return the mean loss.\n","    return tf.reduce_mean(loss)\n","\n","\n","\n","# First class method:\n","class PacketGeneratorSlow(tf.keras.Model):\n","  def __init__(self,vocab_size, categorical_columns, char_id, id_char,\n","               dense_units=512, embed_dim=256, lstm_units=1024, dropout=0.15):\n","    # Initialize parent class:\n","    super().__init__(self)\n","    # save character look up objects:\n","    self.char_id = char_id\n","    self.id_char = id_char\n","    # Defines number of lstm units:\n","    self.units = lstm_units\n","    # Store the one-hot encoding representation of the 32 columns in the dataset:\n","    self.cat_columns = tf.expand_dims(categorical_columns ,axis=0)\n","    # Get the number of columns in the dataset which is 32 in this case:\n","    self.num_of_columns = categorical_columns.shape[0]\n","    # Define the embedding layer:\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embed_dim)\n","    # Define LSTM layer:\n","    self.lstm = tf.keras.layers.LSTM(lstm_units,\n","                                   return_sequences=True,\n","                                   return_state=True)\n","    # Define a second lstm layer\n","    # self.lstm2 = tf.keras.layers.LSTM(lstm_units,\n","    #                                return_sequences=True,\n","    #                                return_state=True)\n","    # Define the 2- layered neural network that encodes the column representation\n","    self.d1 = tf.keras.layers.Dense(256)\n","    self.d2 = tf.keras.layers.Dense(dense_units,)\n","    # Define dropout layer that will be used after the lstm layer\n","    self.drop_out = Dropout(dropout)\n","    #self.dense = tf.keras.layers.Dense(dense_units)\n","    # Define the fully connected layer that makes the prediction for each character at a given time step:\n","    self.fc2 = tf.keras.layers.Dense(vocab_size)\n","\n","  # Define a function that resets the hidden_state of the lstm model every time it is fed a new batch of samples to learn from:\n","  def reset_state(self, batch_size):\n","    return tf.zeros((batch_size, self.units))\n","\n","  # Define the training function: It takes the data set has input and calculates all its requirements dynamically:\n","  @tf.function\n","  def train_step(self,data):\n","    # Define loss variable:\n","    loss = tf.constant(0.0)\n","    # Define loss variable for every column:\n","    category_loss= tf.constant(0.0)\n","    # data shape: (batch size, number of columns, max_length of sequences):\n","    # Get batch_size dynamically from the data fed:\n","    batch_size = tf.shape(data)[0]\n","    # Get the maximum length of the sequences from the data fed:\n","    max_length = tf.shape(data)[-1]\n","    # Create a dynamic column one-hot encoding for each sample in the index batch using the self.cat_colums variable defined during initialization of the model's class:\n","    # batch_categorical shape : (batch_size, number of columns , number of columns )\n","    batch_categorical = tf.repeat(self.cat_columns, batch_size, axis=0)\n","\n","    # Now, train model and update weights with respect to the total_loss per batch.\n","    with tf.GradientTape() as tape:\n","      # Reset hidden state of the lstm model for every batch:\n","      # This ensures that prediction between samples are not dependent.\n","      hidden = self.reset_state(batch_size)\n","      #hidden2 = self.reset_state(batch_size)\n","      # For each column in the batch calculate the encoding representation:\n","      for i in range(self.num_of_columns):\n","        # Each column\n","        column = batch_categorical[:,i]\n","        # Calculate the encoding using the 2-layered network:\n","        logit = self.d1(column)\n","        # logit shape == (batch_size * number of columns, dense units)\n","        logit = self.d2(logit)\n","        #Get the first character in that column for the current batch of samples:\n","        x = data[:,i,0]\n","        # Predict the next character; iterate through all characters:\n","        for j in range(1,max_length):\n","          # Get embedding representation of the index character:\n","          # x shape == (batch_size, 1, embed_dim)\n","          x = self.embedding(x)\n","          # Concatenate the column encoding representation and vector representation of the previous character.\n","          # The 2 will be the input to the lstm model at every time step t:\n","          # x shape == (batch_size, 1, dense_units + lstm_units + embed_dim units)\n","          x = tf.expand_dims(tf.concat((logit, hidden, x), axis=-1) , axis=1)\n","          #  Pass as input to the lstm model: output shape == (batch_size, 1, lstm_units)\n","          output, hidden, cell_state = self.lstm(x)\n","          #output, hidden, _ = self.lstm2(output)\n","          # Use dropout:\n","          output = self.drop_out(output)\n","          #output = self.dense(output)\n","          # Make probability predictions over the vocabulary size using the final fully connected layer given the hidden state output of the lstm layer:\n","          # Predictions shape == (batch_size, vocab_size)\n","          predictions = self.fc2(output)\n","\n","          # Calculate loss on each character and add to the previous value of the loss variable:\n","          loss  = loss + self.loss(data[:,i,j],predictions)\n","\n","          # Using teacher forcing:\n","          # Teacher forcing continually feeds the next correct character to the model\n","          # instead of passing what the model predicted back into the model.\n","          x = data[:, i,j]\n","\n","        # Average the loss over all characters in the current column category.\n","        category_loss = category_loss  + loss/ tf.cast(max_length, tf.float32)\n","      # Calculate total loss over all columns:\n","      total_loss = category_loss/ tf.cast(self.num_of_columns, tf.float32)\n","\n","    # Apply an optimization step\n","    variables = self.trainable_variables \n","    gradients = tape.gradient(total_loss, variables)\n","    # Apply error gradients to the full model:\n","    self.optimizer.apply_gradients(zip(gradients, variables))\n","\n","    # Return a dict mapping metric names to current value\n","    return {'batch_loss': total_loss}\n","\n","# The second method: Parellilizes the computation of the prediction.\n","# It is the same as the one above. The only slight difference is in the train step function\n","# The train step function of this model leads to faster training and data generation.\n","class PacketGeneratorModel(tf.keras.Model):\n","    def __init__(self,vocab_size, categorical_columns, char_id , id_char, \n","                 dense_units=512, embed_dim=256, lstm_units=1024, dropout=0.12):\n","      super().__init__(self)\n","       # save character look up objects:\n","      self.char_id = char_id\n","      self.id_char = id_char\n","      self.units = lstm_units\n","      self.cat_columns = tf.expand_dims(categorical_columns ,axis=0)\n","      self.num_of_columns = categorical_columns.shape[0]\n","      self.embedding = tf.keras.layers.Embedding(vocab_size, embed_dim)\n","      self.lstm = tf.keras.layers.LSTM(lstm_units,\n","                                    return_sequences=True,\n","                                    return_state=True)\n","      # self.lstm2 = tf.keras.layers.LSTM(lstm_units,\n","      #                                return_sequences=True,\n","      #                                return_state=True)\n","      self.d1 = tf.keras.layers.Dense(256, activation='relu')\n","      self.d2 = tf.keras.layers.Dense(dense_units)\n","      self.drop_out = Dropout(dropout)\n","      #self.dense = tf.keras.layers.Dense(dense_units)\n","      self.fc2 = tf.keras.layers.Dense(vocab_size)\n","\n","    def reset_state(self, batch_size):\n","      return tf.zeros((batch_size, self.units))\n","\n","    @tf.function\n","    def train_step(self,data):\n","      #print(data.shape)\n","      # Define loss variable\n","      loss = tf.constant(0.0)\n","      # Define total loss variable\n","      total_loss= tf.constant(0.0)\n","      # data shape: (batch size, number of columns, max_length of sequences):\n","      # Get batch size and maximum sequence length dynamically:\n","      batch_size = tf.shape(data)[0]\n","      max_length = tf.shape(data)[-1]\n","      # batch_categorical shape : (batch_size, number of columns , number of columns )\n","      # Propagate/Extend the one-hot encoding of the columns to fit the number of samples in the batch:\n","      # batch_categorical shape == (batch_size , num of columns , num of columns)\n","      batch_categorical = tf.repeat(self.cat_columns, batch_size, axis=0)\n","      # Reshape for parallelization: new shape == (batch_size * number of columns in the dataset , number of columns in the dataset (one-hot encoding size))\n","      batch_categorical = tf.reshape(batch_categorical, (batch_size * self.num_of_columns, self.num_of_columns))\n","      # Also reshape the batched data for parellization.\n","      # New data shape == (batch_size * number of columns in the dataset, maximum length of sequences):\n","      # This way, the prediction of character sequences in each column can be calculated and optimized at once!\n","      # Data shape == (batch_size * num of columns, maximum length of characters)\n","      data = tf.reshape(data, (batch_size * self.num_of_columns, max_length))\n","\n","      with tf.GradientTape() as tape:\n","        # Reset hidden state of lstm model per batch:\n","        hidden = self.reset_state(batch_size*self.num_of_columns)\n","        #hidden2 = self.reset_state(batch_size)\n","        # Calculate the Encoding representation of all column category in the batch at once:\n","        logit = self.d1(batch_categorical)\n","        # logit shape == (batch_size * number of columns, dense units)\n","        logit = self.d2(logit)\n","        # Get the first character in all colums for all samples in the batch:\n","        x = data[:,0]\n","        # Iterate through subsequent characters whilst predicting the next character and calculating the error loss:\n","        for i in range(1,max_length):\n","          # Embed character : x shape == (batch_size * num of columns, 1, embed_dim)\n","          x = self.embedding(x)\n","          # x shape == (batch_size * num of columns, 1, dense_units + lstm_units + embed_dim)\n","          x = tf.expand_dims(tf.concat((logit, hidden, x), axis=-1) , axis=1)\n","          # Pass to lstm: output shape == (batch_size * num of columns, 1, lstm_units)\n","          output, hidden, _= self.lstm(x)\n","          #output, hidden, _ = self.lstm2(output)\n","          # Dropout some units:\n","          output = self.drop_out(output)\n","          #output = self.dense(output)\n","          # Make predictions on lstm output over vocabulary size:\n","          predictions = self.fc2(output)\n","\n","          # Calculate loss for each index character in all column categories at the same time:\n","          loss  = loss + self.loss(data[:,i],predictions)\n","\n","          # Using teacher forcing:\n","          # Teacher forcing continually feeds the next correct character to the model\n","          # instead of passing what the model predicted back into the model.\n","          x = data[:, i]\n","\n","        # Calculate total loss over all categorical columns:\n","        total_loss = total_loss + loss/ tf.cast(max_length, tf.float32)\n","\n","        # Apply an optimization step\n","        variables = self.trainable_variables \n","        gradients = tape.gradient(total_loss, variables)\n","        self.optimizer.apply_gradients(zip(gradients, variables))\n","\n","        # Return a dict mapping metric names to current value\n","      return {'batch_loss': total_loss}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-r96we-p1EZC"},"source":["## Data Exploration\n","\n","Based on our model architecture, let's get the number of unique values for each columns. This tells the number of columns that can be treated/(modeled by the algorithm) as a categorical feature or 'continous feature'. In this notebook, if a column has less than or equal to 10 exact unique values, it is treated as a categorical feature (generated as a class) and if not, a 'continous feature' (generated as a sequence). we do this to reduce the error that may be gotten during training.\n","\n","For example in the _ws.protocol column that will be treated as a categorical column, instead of the model being trained to generate a full sequence of 'M', 'Q', 'T', 'T' to get 'MQTT', we enforce it to just generate the class 'MQTT' at once!\n","This way, there is only one error generated at prediction in worst case scenario (for example, generating 'TCP' instead of 'MQTT') as opposed to generating a wrong character for each of the character in the sequence 'MQTT'.\n","\n","First, we extract all the column names and store in a list and then, calculate the number of unique values and store it in a dictionary as below.\n","\n","\n","While this value (10) is intuitive, it can be increased or decreased just to optimize model performance in the end. \n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hoCjeNhZ0QHP","executionInfo":{"status":"ok","timestamp":1632610851424,"user_tz":-60,"elapsed":26,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"3ba9dff3-610c-4055-c671-e185a4edfdd6"},"source":["# Get the number of unique values for each column\n","columns = list(packet_data.columns)\n","# Create a dictionary to store the number of unique values per column as key, value pairs:\n","unique_values_per_column = { col : 0 for col in columns}\n","\n","# For each column, calculate and store the number of unique columns:\n","for column in columns : \n","  unique_values_per_column[column] = len(packet_data[column].unique())\n","\n","print(len(columns))\n","unique_values_per_column"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["32\n"]},{"output_type":"execute_result","data":{"text/plain":["{'_ws.col.Protocol': 8,\n"," 'ip.checksum': 52643,\n"," 'ip.dsfield': 19,\n"," 'ip.dst': 23,\n"," 'ip.flags': 3,\n"," 'ip.flags.df': 3,\n"," 'ip.flags.mf': 2,\n"," 'ip.flags.rb': 2,\n"," 'ip.frag_offset': 2,\n"," 'ip.id': 52444,\n"," 'ip.len': 74,\n"," 'ip.proto': 4,\n"," 'ip.src': 21,\n"," 'ip.ttl': 27,\n"," 'tcp.ack': 68,\n"," 'tcp.checksum': 46175,\n"," 'tcp.dstport': 5526,\n"," 'tcp.flags': 10,\n"," 'tcp.flags.ack': 3,\n"," 'tcp.flags.cwr': 2,\n"," 'tcp.flags.fin': 3,\n"," 'tcp.flags.push': 3,\n"," 'tcp.flags.reset': 3,\n"," 'tcp.flags.syn': 3,\n"," 'tcp.flags.urg': 3,\n"," 'tcp.hdr_len': 6,\n"," 'tcp.len': 58,\n"," 'tcp.options.mss_val': 3,\n"," 'tcp.seq': 143,\n"," 'tcp.srcport': 5677,\n"," 'tcp.urgent_pointer': 2,\n"," 'tcp.window_size': 27}"]},"metadata":{},"execution_count":91}]},{"cell_type":"markdown","metadata":{"id":"NRUtbO4qkBgT"},"source":["With this result above, we are able to extract the so-called 'categorical columns' and store them in the variable 'cat_columns' below. We only include the variables that have at most, 10 unique values."]},{"cell_type":"code","metadata":{"id":"n5m8OLhhU_Te"},"source":["cat_columns= ['_ws.col.Protocol', 'ip.flags',  'ip.flags.df',  'ip.flags.mf',  'ip.flags.rb',  'ip.frag_offset',  'ip.proto',\n","               'tcp.flags', 'tcp.flags.ack', 'tcp.flags.cwr', 'tcp.flags.fin', 'tcp.flags.push', 'tcp.flags.reset', 'tcp.flags.syn', 'tcp.flags.urg', \n","              'tcp.hdr_len', 'tcp.options.mss_val', 'tcp.urgent_pointer']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E-vDG4I7AwXf"},"source":["Intuitively speaking, to increase diversity (in the case of the first model class defined above) in the dataset generation, commencing data generation with one of the categorical columns may help increase quality of generation.\n","\n","Therefore, I use 'tcp.flags' as the first column in the dataset. However, this stage can be skipped entirely as it may have very little effect on model performance. For the second model, we are going to skip this phase."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"XISRa8EgzgWI","executionInfo":{"status":"ok","timestamp":1632611675611,"user_tz":-60,"elapsed":384,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"67306451-da61-4896-d833-465ea69711a0"},"source":["index = columns.index('tcp.flags')\n","columns.pop(index)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'tcp.flags'"]},"metadata":{},"execution_count":113}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dLAG45ORMLgN","executionInfo":{"status":"ok","timestamp":1632611678039,"user_tz":-60,"elapsed":347,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"88b91dc9-4c73-4fab-bc3b-f6c7789cc72e"},"source":["index"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["20"]},"metadata":{},"execution_count":114}]},{"cell_type":"code","metadata":{"id":"Q_OT58Njz2M1"},"source":["# Insert the 'tcp.flags' at the first positon of the list.\n","columns.insert(0,'tcp.flags')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8nKbbRdxPU-m","executionInfo":{"status":"ok","timestamp":1632611692712,"user_tz":-60,"elapsed":458,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"89835642-7476-4f28-fc11-e9f94487f373"},"source":["print(columns)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['tcp.flags', 'ip.id', 'ip.flags', 'ip.flags.rb', '_ws.col.Protocol', 'ip.flags.df', 'ip.flags.mf', 'ip.frag_offset', 'ip.ttl', 'ip.proto', 'ip.checksum', 'ip.src', 'ip.dst', 'ip.len', 'ip.dsfield', 'tcp.srcport', 'tcp.dstport', 'tcp.seq', 'tcp.ack', 'tcp.len', 'tcp.hdr_len', 'tcp.flags.fin', 'tcp.flags.syn', 'tcp.flags.reset', 'tcp.flags.push', 'tcp.flags.ack', 'tcp.flags.urg', 'tcp.flags.cwr', 'tcp.window_size', 'tcp.checksum', 'tcp.urgent_pointer', 'tcp.options.mss_val']\n"]}]},{"cell_type":"markdown","metadata":{"id":"04CNrKV5Bj4r"},"source":["Let's get the unique data type of each column in the dataset. It was noticed that all the columns had a datatype of 'float64' except for 10 columns. e.g '_ws.col.protocol', 'ip.flags' etc.\n","\n","We use this understanding to get the different datatypes."]},{"cell_type":"code","metadata":{"id":"CuFBJYk_pwUP"},"source":["# Create a list to store the datatypes of each column:\n","columns_dtype = []\n","for col in columns:\n","  # For each column, if its among the columns with an 'float64' datatype, store 'float64' in the list, else store 'object'\n","  if col not in ('_ws.col.Protocol', 'ip.flags','tcp.flags','ip.id',\n","                 'ip.checksum','ip.src','ip.dst','ip.dsfield', 'tcp.flags','tcp.checksum'):                        \n","    columns_dtype.append( 'float64')\n","  else:\n","    columns_dtype.append('object')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eqh8MZ9cz6g8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632605970915,"user_tz":-60,"elapsed":26,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"ed87e46e-df26-4fbb-ac83-6eb43b252665"},"source":["len(columns)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["32"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"T6RXREiECsEl"},"source":["Let's check the number of empty cells for each column."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GP5AKeDr2C1Z","executionInfo":{"status":"ok","timestamp":1632605971247,"user_tz":-60,"elapsed":346,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"b631fec1-f657-4f66-ab67-9264e9814fcb"},"source":["packet_data.isna().sum()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ip.id                      1\n","ip.flags                   1\n","ip.flags.rb                1\n","_ws.col.Protocol           0\n","ip.flags.df                1\n","ip.flags.mf                1\n","ip.frag_offset             1\n","ip.ttl                     1\n","ip.proto                   1\n","ip.checksum                1\n","ip.src                     1\n","ip.dst                     1\n","ip.len                     1\n","ip.dsfield                 1\n","tcp.srcport             7290\n","tcp.dstport             7290\n","tcp.seq                 7290\n","tcp.ack                 7290\n","tcp.len                 7354\n","tcp.hdr_len             7290\n","tcp.flags               7290\n","tcp.flags.fin           7290\n","tcp.flags.syn           7290\n","tcp.flags.reset         7290\n","tcp.flags.push          7290\n","tcp.flags.ack           7290\n","tcp.flags.urg           7290\n","tcp.flags.cwr           7290\n","tcp.window_size         7290\n","tcp.checksum            7290\n","tcp.urgent_pointer      7290\n","tcp.options.mss_val    80886\n","dtype: int64"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"3aF4_owXzzqU"},"source":["## Preprocess data\n","Some of the samples in the dataset are very incomplete while some have a few missing cells as seen above. The 'tcp.options.mss_val' has about 80,886 missing cells. Therefore to save most of the samples, filling up the null values with their respective mode value will be best here instead of deleting the rows with an empty cell."]},{"cell_type":"code","metadata":{"id":"Me-Q9GnjzFCt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632605972019,"user_tz":-60,"elapsed":775,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"ecdfc7c7-2728-4bd0-c01e-79b395df02c7"},"source":["mode_values = { col : packet_data[col].mode()[0] for col in columns}\n","for column in columns :\n","  packet_data[column].fillna(value= mode_values[column], inplace=True)\n","  \n","packet_data.isna().sum()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ip.id                  0\n","ip.flags               0\n","ip.flags.rb            0\n","_ws.col.Protocol       0\n","ip.flags.df            0\n","ip.flags.mf            0\n","ip.frag_offset         0\n","ip.ttl                 0\n","ip.proto               0\n","ip.checksum            0\n","ip.src                 0\n","ip.dst                 0\n","ip.len                 0\n","ip.dsfield             0\n","tcp.srcport            0\n","tcp.dstport            0\n","tcp.seq                0\n","tcp.ack                0\n","tcp.len                0\n","tcp.hdr_len            0\n","tcp.flags              0\n","tcp.flags.fin          0\n","tcp.flags.syn          0\n","tcp.flags.reset        0\n","tcp.flags.push         0\n","tcp.flags.ack          0\n","tcp.flags.urg          0\n","tcp.flags.cwr          0\n","tcp.window_size        0\n","tcp.checksum           0\n","tcp.urgent_pointer     0\n","tcp.options.mss_val    0\n","dtype: int64"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"0lrhz5BzDU1q"},"source":["Empty cells have been successfully dealt with. Now, we create a one hot encoding of the different columns in the dataset."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2xAPH9CXAv-7","executionInfo":{"status":"ok","timestamp":1632605972020,"user_tz":-60,"elapsed":11,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"ed0446b0-5ac9-4a10-99c4-743a47d86e34"},"source":["one_hot_columns = tf.keras.utils.to_categorical(range(len(columns)))\n","one_hot_columns"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1., 0., 0., ..., 0., 0., 0.],\n","       [0., 1., 0., ..., 0., 0., 0.],\n","       [0., 0., 1., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 1., 0., 0.],\n","       [0., 0., 0., ..., 0., 1., 0.],\n","       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"D_Ac61OMpvbM"},"source":["Now, we preprocess the dataset by creating a list of columns. Each column is a list of the containing values across the rows in the dataset. \n","\n","In the process, we also calculate the maximum length of sequence in the code below. we also extract  the unique values (for both 'categorical' or 'continous' columns) that make up the vocabulary. We also prepend '<' and append '>' to each values in the respective columns. They will serve as the 'start' and 'end' prompt during training."]},{"cell_type":"code","metadata":{"id":"RITuBwpqCIpb"},"source":["# Create variable to hold the maximum length of sequences:\n","max_length = 0\n","# Define the vocabulary container:\n","vocab = set()\n","# list of columns:\n","train_data = [[] for i in range(len(columns))]\n","\n","# This function does not treat the categorical column values as just one whole character:\n","def extract_data(column, col, index, max_len):\n","  # For each column in dataset, prepend '<' and append '>' to each unique values.\n","  for each in col:\n","    # change to a list of characters first:\n","    char_list = list(str(each))\n","    # prepend '<' and append '>' to the list\n","    char_list.insert(0,'<')\n","    char_list.append('>')\n","    # Extract unique characters and store them in the vocabulary:\n","    for i in char_list:\n","      vocab.add(i)\n","    # Store processed values appropriately in the list of columns container.\n","    train_data[index].append(char_list)\n","    # Get the maximum length of sequences accross all rows and columns:\n","    max_len = max(max_len, len(char_list))\n","  # Return the maximum length:\n","  return max_len\n","    \n","# This function treats the categorical column values as one whole character as discussed earlier:\n","def extract_data2(column, col, index, max_len):\n","  # If column is a categorical column, do this:\n","  if column in cat_columns:\n","    # Just append '>' and prepend '<' to the value and store in the list of columns container:\n","    for each in col:\n","        temp = []\n","        temp.append('<')\n","        temp.append(str(each))\n","        temp.append('>')\n","        train_data[index].append(temp)\n","        #items_list.append(each)\n","        # Store unique values in vocabulary container.\n","        vocab.add(str(each))\n","  # Else:\n","  else:\n","    # Else just do exactly what the first function does.\n","    for each in col:\n","      char_list = list(str(each))\n","      char_list.insert(0,'<')\n","      char_list.append('>')\n","      for i in char_list:\n","        vocab.add(i)\n","      train_data[index].append(char_list)\n","      max_len = max(max_len, len(char_list))\n","  return max_len\n","  \n","# Generate processed dataset here:\n","for index, column in enumerate(columns):\n","  max_length = extract_data2(column,packet_data[column],index, max_length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eS_7Gd3NheVL","executionInfo":{"status":"ok","timestamp":1632605988607,"user_tz":-60,"elapsed":26,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"f451df87-11c1-4911-a511-ff7be4bacead"},"source":["len(vocab)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["50"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"Or5RwMHwJNxy"},"source":["We see the maximum_length is 14 as below and the number of unique characters in the dataset is 50. Let's examine the contents of the vocabulary. In this notebook, no foreign letter/character was added to the dataset. All characters in vocabulary were gotten from the dataset.\n","\n","Perhaps, some letters can be added to increase dynamics of generation."]},{"cell_type":"code","metadata":{"id":"OsxCVkQNyjS3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632605988968,"user_tz":-60,"elapsed":11,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"1df1118f-4264-4b0b-8a63-95932fb1d7d0"},"source":["max_length, len(vocab)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(14, 50)"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6sceDc-pcxHk","executionInfo":{"status":"ok","timestamp":1632613563374,"user_tz":-60,"elapsed":31,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"69156fbd-7c4b-44c3-b4a6-c699e21a56e8"},"source":["#{'.',  '0',  '0.0',  '0x00000000',  '0x00000002',  '0x00000004',  '0x00000010',  '0x00000011',   '0x00000012',  '0x00000014',  '0x00000018',  '0x00000019',\n","# '0x00000029',  '0x00004000',  '1',  '1.0',  '1460.0',  '17.0',  '2',  '20.0',  '24.0',  '265.0',  '3',  '32.0',  '4',  '40.0',  '44.0',  '5',  '6',  '6.0',\n","# '7',  '8',  '9',  '<',  '>',  'DNS',  'ICMP',  'MDNS',  'MPEG PES',  'MPEG TS',  'MQTT',  'TCP',  'UDP',  'a',  'b',  'c',  'd',  'e',  'f',  'x'}\n","\n","\n","print(vocab)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'17.0', 'f', '20.0', '6', '0x00000012', 'TCP', '0x00000011', '0x00004000', 'MPEG TS', '32.0', '4', 'MDNS', '0x00000004', '<', 'a', 'ICMP', '24.0', 'd', '0x00000019', '9', '3', '0x00000000', '6.0', '1460.0', 'c', '0x00000010', '40.0', '265.0', 'UDP', '2', '0', 'MPEG PES', '>', '0.0', 'x', 'MQTT', '44.0', '5', '1.0', 'b', '0x00000029', '0x00000002', '0x00000018', '0x00000014', 'e', '7', '8', '1', '.', 'DNS'}\n"]}]},{"cell_type":"markdown","metadata":{"id":"0DqDVGcoJ7l9"},"source":["Next, we define 2 separate stringlookup class object that (1) convert the characters to unique integer values and (2) convert these unique integer values to their respective characters. we also add a pad value (i.e mask_token) which represents the padding character for all the sequences.\n","\n","This is done because machine learning models can only understand digital values or numbers and not strings during their computation."]},{"cell_type":"code","metadata":{"id":"VKbxh33b0KDv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632606001217,"user_tz":-60,"elapsed":363,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"fea80c0e-f971-4efb-e7ec-d21def4e2835"},"source":["# object converts characters to indexes.\n","ids_from_chars = tf.keras.layers.StringLookup(\n","    vocabulary=list(vocab), mask_token='<p>')\n","\n","# object converts unique integers to their respective characters.\n","chars_from_ids = tf.keras.layers.StringLookup(\n","    vocabulary=list(vocab), invert=True, mask_token='<p>')\n","\n","print('MQTT :- ',ids_from_chars(['MQTT']).numpy()[0])\n","print(' 37 :- ', chars_from_ids([37]).numpy()[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MQTT :-  37\n"," 37 :-  b'MQTT'\n"]}]},{"cell_type":"markdown","metadata":{"id":"QKSMGXfINQ5c"},"source":["Next, we pad each sequence with a unique value so that they all have the same length."]},{"cell_type":"code","metadata":{"id":"9ClrsT3L6DmM"},"source":["# For each columns in the train_data:\n","for column in train_data:\n","  for each in column:\n","    # Calculate the length of each value \n","    len_chars = len(each)\n","    # If the calculated length is less than the maximum length, pad the sequence with '<p>':\n","    if len_chars < max_length:\n","          for i in range(max_length - len_chars):\n","            each.append('<p>')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ILBVtqtsxGVc","executionInfo":{"status":"ok","timestamp":1632606023944,"user_tz":-60,"elapsed":6403,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"fdaa1a7c-726d-406a-8bf8-cbb370920824"},"source":["train_data = np.array(train_data)\n","train_data = np.transpose(train_data,(1,0,2))\n","train_data.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(112373, 32, 14)"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d2X0UsuEyc1e","executionInfo":{"status":"ok","timestamp":1632606036046,"user_tz":-60,"elapsed":12108,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"001376f0-16e9-4a6d-9e95-62ab33b23d65"},"source":["# convert the dataset to their respective integer values:\n","data = ids_from_chars(train_data)\n","data.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([112373, 32, 14])"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"LdidTiU9PY4W"},"source":["Let's examine a random row to see what the preprocessed data looks like:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J6qAF1fs3QJU","executionInfo":{"status":"ok","timestamp":1632606036047,"user_tz":-60,"elapsed":24,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"05b214ca-def3-4151-bb25-df54e4e23b24"},"source":["train_data[55004]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([['<', '0x00000010', '>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>', '<p>', '<p>'],\n","       ['<', '0', 'x', '0', '0', '0', '0', 'e', 'c', '8', 'b', '>',\n","        '<p>', '<p>'],\n","       ['<', '0x00004000', '>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>', '<p>', '<p>'],\n","       ['<', '0.0', '>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>', '<p>'],\n","       ['<', 'TCP', '>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>', '<p>'],\n","       ['<', '1.0', '>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>', '<p>'],\n","       ['<', '0.0', '>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>', '<p>'],\n","       ['<', '0.0', '>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>', '<p>'],\n","       ['<', '6', '4', '.', '0', '>', '<p>', '<p>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>'],\n","       ['<', '6.0', '>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>', '<p>'],\n","       ['<', '0', 'x', '0', '0', '0', '0', '8', '2', '8', '4', '>',\n","        '<p>', '<p>'],\n","       ['<', '1', '0', '.', '0', '.', '0', '.', '5', '>', '<p>', '<p>',\n","        '<p>', '<p>'],\n","       ['<', '1', '9', '2', '.', '1', '6', '8', '.', '1', '.', '7', '>',\n","        '<p>'],\n","       ['<', '5', '2', '.', '0', '>', '<p>', '<p>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>'],\n","       ['<', '0', 'x', '0', '0', '0', '0', '0', '0', '0', '0', '>',\n","        '<p>', '<p>'],\n","       ['<', '5', '9', '1', '9', '8', '.', '0', '>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>'],\n","       ['<', '1', '8', '8', '3', '.', '0', '>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>'],\n","       ['<', '7', '4', '.', '0', '>', '<p>', '<p>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>'],\n","       ['<', '6', '.', '0', '>', '<p>', '<p>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>', '<p>'],\n","       ['<', '0', '.', '0', '>', '<p>', '<p>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>', '<p>'],\n","       ['<', '32.0', '>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>', '<p>', '<p>'],\n","       ['<', '0.0', '>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>', '<p>'],\n","       ['<', '0.0', '>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>', '<p>'],\n","       ['<', '0.0', '>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>', '<p>'],\n","       ['<', '0.0', '>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>', '<p>'],\n","       ['<', '1.0', '>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>', '<p>'],\n","       ['<', '0.0', '>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>', '<p>'],\n","       ['<', '0.0', '>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>', '<p>'],\n","       ['<', '2', '9', '2', '0', '0', '.', '0', '>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>'],\n","       ['<', '0', 'x', '0', '0', '0', '0', '0', 'f', '5', '6', '>',\n","        '<p>', '<p>'],\n","       ['<', '0.0', '>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>', '<p>'],\n","       ['<', '1460.0', '>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>',\n","        '<p>', '<p>', '<p>', '<p>', '<p>']], dtype='<U10')"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"DnyaLL1tPkEy"},"source":["## Define and Train Model\n","\n","Now let's initialize and train a model from the second model class definition. We then evaluate the performance intuitively. We train the second model first as it is fast and with less overheads during training and random generation."]},{"cell_type":"code","metadata":{"id":"T69HDXVgy02w"},"source":["# Define number of epochs for both kind of model:\n","epochs_model1 = 20\n","epochs_model2 = 2\n","TRAIN_BUFFER_SIZE = len(data)\n","BATCH_SIZE = 256\n","\n","\n","# This Dataset class shuffles the data and precreates batches for it.\n","dataset = tf.data.Dataset.from_tensor_slices(data).shuffle(TRAIN_BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE)\n","dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n7f4rSaRM2Xq"},"source":["# Initialize model from the second class definition: Pass the vocabulary size and the one_hot_encoded column representation.\n","model = PacketGeneratorModel(len(ids_from_chars.get_vocabulary()), one_hot_columns, ids_from_chars, chars_from_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ANbbg9FpFtF8"},"source":["# Compile model with the respective loss class and optimizer:\n","model.compile(\n","    optimizer=tf.optimizers.Adam(),\n","    loss=MaskedLoss(),\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d-QzvzIrH_R7"},"source":["# path to save model weight checkpoint\n","checkpoint_path = os.path.join(FILE_PATH, \"Train2\")\n","# ckpt = tf.train.Checkpoint(model)\n","# ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n","\n","# start_epoch = 0\n","# if ckpt_manager.latest_checkpoint:\n","#   # If you retrain this model then uncomment the next line of code:\n","#   #start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n","#   start_epoch = int(ckpt_manager.checkpoints[0].split('-')[-1])\n","\n","#   # restoring the latest checkpoint in checkpoint_path\n","#   ckpt.restore(ckpt_manager.latest_checkpoint)\n","# #ckpt.restore(ckpt_manager.checkpoints[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aOn_6Nj8Znr5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632606109456,"user_tz":-60,"elapsed":7,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"f9b24819-9230-4d85-92e3-72b2d0f47a44"},"source":["# Load previous model weights from directory\n","model.load_weights(checkpoint_path)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f0e84615dd0>"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"1HPDq9-kKwpE"},"source":["# Define a model checkpoint class from keras for this model training.\n","# It saves the model weights in the checkpoint path after every batch iteration:\n","cp = tf.keras.callbacks.ModelCheckpoint(\n","    checkpoint_path, monitor='batch_loss', verbose=0, save_best_only=True,\n","    save_weights_only=True, mode='min', save_freq= BATCH_SIZE\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"GT368d6AM9N5","executionInfo":{"status":"error","timestamp":1632608223950,"user_tz":-60,"elapsed":2109819,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"65b98adc-94b5-4f4a-9107-0eadcbb108c1"},"source":["# Train the model \n","model.fit(dataset,epochs=epochs_model2, callbacks=[cp])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","439/439 [==============================] - 1894s 4s/step - batch_loss: 0.1235\n","Epoch 2/2\n"," 37/439 [=>............................] - ETA: 28:32 - batch_loss: 0.0778"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-649b31e21358>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs_model2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"kQ6R4WKfSgBF"},"source":["While training the model above, it was noticed that loss was < 0.1 during the second epoch training so it was stopped as seen above. The first kind of model can also be trained with the code below. However, it was ignored because of the length of time it took to train (it took about 45 mins to train 1 epoch with a total loss of about 5.2) and slower/poor optimization as compared to the first given the GPU usage limit imposed on notebooks.\n","\n","To train the model, just run the next 2 cells below.\n","\n","However, we focus on the above trained model for the rest of the notebook."]},{"cell_type":"code","metadata":{"id":"T0ZUjsKFDzB9"},"source":["# model = PacketGeneratorModel(len(ids_from_chars.get_vocabulary()), one_hot_columns, ids_from_chars, chars_from_ids)\n","# #model = PacketGeneratorModel(len(ids_from_chars.get_vocabulary()), one_hot_columns)\n","# model.compile(\n","#     optimizer=tf.optimizers.Adam(),\n","#     loss=MaskedLoss(),\n","# )\n","# checkpoint_path = os.path.join(FILE_PATH, \"Train\")\n","# #model.load_weights(checkpoint_path)\n","# cp = tf.keras.callbacks.ModelCheckpoint(\n","#     checkpoint_path, monitor='batch_loss', verbose=0, save_best_only=True,\n","#     save_weights_only=True, mode='min', save_freq= BATCH_SIZE\n","# )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QLdcQ2DgEL0n"},"source":["\n","#model.fit(dataset,epochs=epochs_model1, callbacks=[cp])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kkiBM-ab3wXv"},"source":["# Packet generator for model:\n","class PacketGenerator(tf.Module):\n","  def __init__(self, model, column_names, columns_dtype):\n","    super(PacketGenerator, self).__init__()\n","    # Initialize model, ids_from_chars, chars_from_ids, cat_columns , num_of_columns, column_names, columns_dtype:\n","    self.model = model\n","    self.char_id = model.char_id\n","    self.id_char = model.id_char\n","    self.cat_columns = self.model.cat_columns\n","    self.num_of_columns = self.model.num_of_columns\n","    self.column_names = column_names\n","    #self.cat_column_names = cat_column_names\n","    self.columns_dtype = columns_dtype\n","\n","    # The output should never generate padding, unknown, or start.\n","    # Therefore, get the index of '<p>', '[UNK]' and '[START]' from the vocabulary:\n","    token_mask_ids = self.char_id(['<p>', '[UNK]', '<',]).numpy()\n","\n","    # Create a token mask by first creating an array of tokens (vocabulary) and then initialize all to False\n","    # Set the indexes of the '', '[UNK]' and '<' to True:\n","    token_mask = np.zeros([self.char_id.vocabulary_size()], dtype=np.bool)\n","    token_mask[np.array(token_mask_ids)] = True\n","\n","    # set as a property of the class\n","    self.token_mask = token_mask\n","\n","    # Get the index of the '[START]' and '[END]' tokens:\n","    \n","    self.start_token = self.char_id(['<']).numpy()[0]\n","    self.end_token = self.char_id(['>']).numpy()[0]\n","\n","  #@tf.function\n","  def tokens_to_text(self, result_tokens):\n","    # This method converts index tokens to the string tokens by using the stringlookup we initialized in the init() method:\n","    result_text_tokens = self.id_char(result_tokens)\n","\n","    return result_text_tokens#.numpy()\n","\n","  #@tf.function\n","  def sample(self, logits, temperature):\n","\n","    # Add 2 new axis to the token_mask shape\n","    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n","\n","    # Set the logits for all masked tokens ('','[START]', '[UNK]') to -inf, so they are never chosen.\n","    # The logits (shape== (batch_size, vocab_size) here represents the independent probabilities of each word in the vocabulary.\n","    logits = tf.where(token_mask, -np.inf, logits)\n","    # If temperature is 0, get the max logit argument for each sample in the batch:\n","    # This argument represents the index of the word with the highest independent probability.\n","    if temperature == 0.0:\n","      new_tokens = tf.argmax(logits, axis=-1)\n","    # Else, sample from the independent probabilities of the logit:\n","    else: \n","      logits = tf.squeeze(logits, axis=1)\n","      new_tokens = tf.random.categorical(logits/temperature,\n","                                          num_samples=1)\n","    return new_tokens\n","    \n","  @tf.function\n","  def generate_samples(self,batch_size=500, max_length=14, temperature=1.0):\n","\n","    batch_categorical = tf.repeat(self.cat_columns, batch_size, axis=0)\n","    batch_categorical = tf.reshape(batch_categorical, (batch_size * self.num_of_columns, self.num_of_columns))\n","\n","    # Initialize the accumulators for the decoder output:\n","    #result_tokens = tf.TensorArray()\n","    \n","    hidden = self.model.reset_state(batch_size*self.num_of_columns)\n","    logit = self.model.d1(batch_categorical)\n","    logit = self.model.d2(logit)\n","    #x = data[:,i,0]\n","    # Initialize\n","    new_tokens = tf.fill((batch_size * self.num_of_columns,), self.start_token)\n","\n","    # Initialize a tf array that tracks the end_token of all samples in the batch:\n","    done = tf.zeros([batch_size*self.num_of_columns, 1], dtype=tf.bool)\n","\n","    # Initialize the accumulators for the decoder output:\n","    categorical_tokens = tf.TensorArray(tf.int64, size=1, dynamic_size=True)\n","\n","    for t in tf.range(1,max_length):\n","      # Embed the index character:\n","      x = self.model.embedding(new_tokens)\n","      # Concatenate the logit, previous hidden state and index embedding representaton x:\n","      x = tf.expand_dims(tf.concat((logit, hidden, x), axis=-1) , axis=1)\n","      # Pass result through the lstm model.\n","      output, hidden, _ = self.model.lstm(x)\n","      #output = self.dense(output)\n","      predictions = self.model.fc2(output)\n","\n","      new_tokens = self.sample(predictions, temperature)\n","      #print('After self.sample',new_tokens.shape)\n","      # If a sequence produces an `end_token`, set it `done`\n","      done = done | (new_tokens == self.end_token)\n","      # Once a sequence is done it only produces 0-padding.\n","      new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n","\n","      # Collect the generated tokens\n","      categorical_tokens = categorical_tokens.write(t, new_tokens)\n","      new_tokens = tf.squeeze(new_tokens)\n","\n","\n","      if tf.reduce_all(done):\n","        break\n","\n","    # Convert the list of generated token ids to a list of strings.\n","    categorical_tokens = categorical_tokens.stack()\n","    #cate\n","    categorical_tokens = tf.squeeze(categorical_tokens, -1)\n","    categorical_tokens = tf.transpose(categorical_tokens, [1, 0])\n","    # change shape of categorical tokens:\n","    #print(categorical_tokens.shape)\n","    categorical_tokens = tf.reshape(categorical_tokens, (batch_size, self.num_of_columns, -1))\n","    result_text_array = self.tokens_to_text(categorical_tokens)\n","\n","\n","    return result_text_array\n","\n","\n","def create_tabular_data(dataset, column_names, tcp_flags_index = 20, file_name = 'Model_packet_data_output', save=False):\n","  print('Creating tabular data...')\n","  # Create a pandas dataframe from the processed dataset:\n","  dataframe = pd.DataFrame(dataset, columns= column_names)\n","  # Restore to natural order of columns to match with the original dataset:\n","  column  = column_names.pop(0)\n","  # Take the column name 'tcp.flags' to its original position in the dataset:\n","  column_names.insert(tcp_flags_index,column)\n","  # Restore\n","  dataframe = dataframe[column_names]\n","  # Decode all strings in each column from bytes to 'utf-8':\n","  for col in column_names:\n","    dataframe[col] = dataframe[col].str.decode('utf-8')\n","  # If save , save to the provided file.\n","  print(file_name)\n","  if save:\n","    dataframe.to_csv(file_name)\n","    return dataframe\n","  else:\n","    return dataframe\n","\n","def process_data(dataset):\n","  # Replace all occurences of '>', '<', '<p>' with ''.\n","  dataset = tf.strings.regex_replace(dataset, b'<p>', b'')\n","  dataset = tf.strings.regex_replace(dataset, b'>', b'')\n","  dataset = tf.strings.regex_replace(dataset, b'<', b'')\n","  # In the innermost axis, join all the elements of the list together to form one long string:\n","  dataset = tf.strings.reduce_join(dataset, axis = -1)\n","  \n","  \n","  return dataset.numpy()\n","\n","def generate(model,data_size, batch_size=2000, temperature=1.0, save=False, file_name='packet_data_output.csv'):\n","  print('Generating packet data ...')\n","  # If data_size < batch_size, make data_size the batch_size:\n","  if data_size < batch_size:\n","    data = model.generate_samples(batch_size=data_size, temperature =temperature)\n","  else:\n","    # Else use the batch_size to generate subsequent batches of generated datasets till the data_size is met:\n","    incorrect_size = True\n","    data = None\n","    while (incorrect_size):\n","      data = model.generate_samples(batch_size=batch_size, temperature =temperature)\n","      if data.shape[-1] < 14:\n","        continue\n","      else:\n","        incorrect_size = False\n","    for i in range(batch_size, data_size, batch_size):\n","      unequal_size = True\n","      while (unequal_size):\n","        temp_data = model.generate_samples(batch_size=batch_size, temperature=temperature)\n","        if temp_data.shape[-1] < 14:\n","          continue\n","        else:\n","          data  = tf.concat([data, temp_data], axis=0)\n","          unequal_size = False\n","\n","  # process_data: remove '>', '<', '<p>' strings from all items:\n","  data = process_data(data)\n","  #print(data.shape)\n","  data = create_tabular_data(data, model.column_names, file_name, save=save)\n","  return data\n","\n","# Function wraps the model in a class for tabular data generation:\n","def get_data_generator(model, column_names, columns_dtype):\n","  return PacketGenerator(model,  column_names, columns_dtype)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"em03bSsMG96y"},"source":["Save the column names and their respective data types to local storage."]},{"cell_type":"code","metadata":{"id":"qtiK3MPGFk8n"},"source":["import pickle\n","\n","with open('columns.pkl', 'wb') as f:\n","  pickle.dump(columns,f)\n","\n","with open('column_dtypes.pkl', 'wb') as f:\n","  pickle.dump(columns_dtype, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C91uETWQkCQg"},"source":["# Create an instance of the translator model:\n","data_generator = get_data_generator(model,  columns, columns_dtype)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gYUcsCKLTJAu"},"source":["Let's save the trained model along with all its dependencies to our local directory using the tf.saved_model.save() function."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vj6oOVz0d3sI","executionInfo":{"status":"ok","timestamp":1632608470786,"user_tz":-60,"elapsed":6822,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"44171345-642b-442f-86a0-0fc0b0964cc8"},"source":["tf.saved_model.save(data_generator, 'packet_generator2')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.PacketGeneratorModel object at 0x7f0e70385150>, because it is not built.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.PacketGeneratorModel object at 0x7f0e70385150>, because it is not built.\n","WARNING:absl:Found untraced functions such as embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dense_3_layer_call_fn, dense_3_layer_call_and_return_conditional_losses, dense_4_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: packet_generator2/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: packet_generator2/assets\n"]}]},{"cell_type":"code","metadata":{"id":"OUt3CnWIeKIo"},"source":["m = tf.saved_model.load('packet_generator2')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-_tWEKevTaMo"},"source":["The following code below defines the customized function (and dependencies) that will be used by the saved trained model (saved via tf.saved_model)."]},{"cell_type":"code","metadata":{"id":"SGt2oROLHa6N"},"source":["# Function that loads column names from memory:\n","def get_column_names():\n","  with open('columns.pkl','rb') as f:\n","    columns = pickle.load(f)\n","  return columns\n","\n","# Function that load the column datatypes from storage:\n","def get_columns_dtype():\n","  with open('column_dtypes.pkl', 'rb') as f:\n","    columns_dtype = pickle.load(f)\n","  return columns_dtype\n","\n","def generate(model,data_size, column_names=None, batch_size=2000, temperature=1.0, save=False, file_name='packet_data_output.csv'):\n","  print('Generating packet data ...')\n","  '''\n","  '''\n","  # If batch_size is not 100, 500, 1000, 0r 2000, print the below:\n","  if batch_size not in [100, 500, 1000, 2000]:\n","      print('Model only takes batch sizes of : 100, 500, 1000, 2000. Please, use one of these predefined sizes.')\n","      return \n","  # If data_size < batch_size, make data_size the batch_size:\n","  if data_size < batch_size:\n","    try:\n","      data = model.generate_samples(batch_size=data_size, temperature =temperature)\n","    except:\n","      print('Model only takes batch sizes of : 100, 500, 1000, 2000. Please, use one of these predefined sizes or a size larger and that is a multiple of the batch size used.')\n","      return\n","  else:\n","    # Else use the batch_size to generate subsequent batches of generated datasets till the data_size is met:\n","    incorrect_size = True\n","    data = None\n","    while (incorrect_size):\n","      data = model.generate_samples(batch_size=batch_size, temperature =temperature)\n","      if data.shape[-1] < 14:\n","        continue\n","      else:\n","        incorrect_size = False\n","    for i in range(batch_size, data_size, batch_size):\n","      unequal_size = True\n","      while (unequal_size):\n","        temp_data = model.generate_samples(batch_size=batch_size, temperature=temperature)\n","        if temp_data.shape[-1] < 14:\n","          continue\n","        else:\n","          data  = tf.concat([data, temp_data], axis=0)\n","          unequal_size = False\n","\n","  # process_data: remove '>', '<', '<p>' strings from all items:\n","  data = process_data(data)\n","  if column_names == None:\n","    column_names = get_column_names()\n","    data = create_tabular_data(data, column_names=column_names, file_name=file_name, save=save)\n","  else:\n","    data = create_tabular_data(data, column_names=column_names, file_name=file_name, save=save)\n","  return data\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M6gc0h8HUxRS"},"source":["Now, we use the model to generate 100,000 samples below and we also record the time it took to generate these 100,000 samples."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":264},"id":"0KBCGg2ttMOL","executionInfo":{"status":"ok","timestamp":1632610627171,"user_tz":-60,"elapsed":578447,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"aceee6a8-d6de-46bb-aecc-640f5ba31892"},"source":["n_samples= 100000\n","# Get the start time\n","start_time = time.time()\n","# Generate data:\n","#check = data_generator.generate(n_samples,save=True ,temperature=1.0)\n","check = generate(m, n_samples, columns, save=True ,temperature=1.0)\n","# print(check.shape)\n","# check.head()\n","print(f'Time taken to generate {n_samples} is : {(time.time() - start_time)/60} min(s)')\n","check[:5]\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generating packet data ...\n","Creating tabular data...\n","packet_data_output.csv\n","Time taken to generate 100000 is : 9.640553319454193 min(s)\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tcp.flags</th>\n","      <th>ip.id</th>\n","      <th>ip.flags</th>\n","      <th>ip.flags.rb</th>\n","      <th>_ws.col.Protocol</th>\n","      <th>ip.flags.df</th>\n","      <th>ip.flags.mf</th>\n","      <th>ip.frag_offset</th>\n","      <th>ip.ttl</th>\n","      <th>ip.proto</th>\n","      <th>ip.checksum</th>\n","      <th>ip.src</th>\n","      <th>ip.dst</th>\n","      <th>ip.len</th>\n","      <th>ip.dsfield</th>\n","      <th>tcp.srcport</th>\n","      <th>tcp.dstport</th>\n","      <th>tcp.seq</th>\n","      <th>tcp.ack</th>\n","      <th>tcp.len</th>\n","      <th>tcp.hdr_len</th>\n","      <th>tcp.flags.fin</th>\n","      <th>tcp.flags.syn</th>\n","      <th>tcp.flags.reset</th>\n","      <th>tcp.flags.push</th>\n","      <th>tcp.flags.ack</th>\n","      <th>tcp.flags.urg</th>\n","      <th>tcp.flags.cwr</th>\n","      <th>tcp.window_size</th>\n","      <th>tcp.checksum</th>\n","      <th>tcp.urgent_pointer</th>\n","      <th>tcp.options.mss_val</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0x00000012</td>\n","      <td>0x0000d40b</td>\n","      <td>0x00000000</td>\n","      <td>0.0</td>\n","      <td>TCP</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>33.0</td>\n","      <td>6.0</td>\n","      <td>0x00005afa</td>\n","      <td>10.0.0.15</td>\n","      <td>192.168.2.5</td>\n","      <td>44.0</td>\n","      <td>0x00000000</td>\n","      <td>46192.0</td>\n","      <td>35087.0</td>\n","      <td>50.0</td>\n","      <td>164.0</td>\n","      <td>4.0</td>\n","      <td>32.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>29200.0</td>\n","      <td>0x0000a73d</td>\n","      <td>0.0</td>\n","      <td>1460.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0x00000002</td>\n","      <td>0x000064b2</td>\n","      <td>0x00004000</td>\n","      <td>0.0</td>\n","      <td>TCP</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>63.0</td>\n","      <td>6.0</td>\n","      <td>0x000065e1</td>\n","      <td>192.168.1.7</td>\n","      <td>192.168.2.7</td>\n","      <td>52.0</td>\n","      <td>0x00000000</td>\n","      <td>35087.0</td>\n","      <td>1883.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>40.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1024.0</td>\n","      <td>0x000037b5</td>\n","      <td>0.0</td>\n","      <td>1460.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0x00000010</td>\n","      <td>0x0000d927</td>\n","      <td>0x00004000</td>\n","      <td>0.0</td>\n","      <td>TCP</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>64.0</td>\n","      <td>6.0</td>\n","      <td>0x0000271d</td>\n","      <td>10.0.0.23</td>\n","      <td>10.0.0.7</td>\n","      <td>56.0</td>\n","      <td>0x00000000</td>\n","      <td>59992.0</td>\n","      <td>56756.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>20.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1024.0</td>\n","      <td>0x00001bc1</td>\n","      <td>0.0</td>\n","      <td>1460.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0x00000010</td>\n","      <td>0x0000588f</td>\n","      <td>0x00004000</td>\n","      <td>0.0</td>\n","      <td>TCP</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>64.0</td>\n","      <td>6.0</td>\n","      <td>0x00000c3d</td>\n","      <td>192.168.1.7</td>\n","      <td>892.168.1.7</td>\n","      <td>52.0</td>\n","      <td>0x00000000</td>\n","      <td>1883.0</td>\n","      <td>3824.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>24.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>29200.0</td>\n","      <td>0x00008892</td>\n","      <td>0.0</td>\n","      <td>1460.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0x00000010</td>\n","      <td>0x0000b502</td>\n","      <td>0x00004000</td>\n","      <td>0.0</td>\n","      <td>TCP</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>63.0</td>\n","      <td>6.0</td>\n","      <td>0x00008c35</td>\n","      <td>192.168.2.5</td>\n","      <td>192.168.2.5</td>\n","      <td>101.0</td>\n","      <td>0x00000000</td>\n","      <td>1883.0</td>\n","      <td>1730.0</td>\n","      <td>0.0</td>\n","      <td>74.0</td>\n","      <td>0.0</td>\n","      <td>20.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1024.0</td>\n","      <td>0x0000cfe1</td>\n","      <td>0.0</td>\n","      <td>1460.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    tcp.flags       ip.id  ... tcp.urgent_pointer tcp.options.mss_val\n","0  0x00000012  0x0000d40b  ...                0.0              1460.0\n","1  0x00000002  0x000064b2  ...                0.0              1460.0\n","2  0x00000010  0x0000d927  ...                0.0              1460.0\n","3  0x00000010  0x0000588f  ...                0.0              1460.0\n","4  0x00000010  0x0000b502  ...                0.0              1460.0\n","\n","[5 rows x 32 columns]"]},"metadata":{},"execution_count":80}]},{"cell_type":"markdown","metadata":{"id":"svN620ZUisBB"},"source":["It took about 9.6 mins for this model to generate a dataset of 100,000 samples. Also, some minor errors may be noted in the model's output. This is because no model can be 100% optimized in most cases.\n","\n","Let's create another 150,000 samples"]},{"cell_type":"code","metadata":{"id":"iAgxFLGsUpou","colab":{"base_uri":"https://localhost:8080/","height":264},"executionInfo":{"status":"ok","timestamp":1632613298687,"user_tz":-60,"elapsed":854653,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"f22b1d1f-137e-48c2-c9be-a294712f4076"},"source":["n_samples= 150000\n","# Get the start time\n","start_time = time.time()\n","# Generate data:\n","#check = data_generator.generate(n_samples,save=True ,temperature=1.0)\n","new_samples = generate(m, 150000, columns, save=True ,temperature=1.0, file_name='model_packet_data_output2.csv')\n","# print(check.shape)\n","# check.head()\n","print(f'Time taken to generate {n_samples} is : {(time.time() - start_time)/60} min(s)')\n","new_samples[:5]\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generating packet data ...\n","Creating tabular data...\n","model_packet_data_output2.csv\n","Time taken to generate 150000 is : 14.23298218647639 min(s)\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ip.flags</th>\n","      <th>ip.flags.rb</th>\n","      <th>_ws.col.Protocol</th>\n","      <th>ip.flags.df</th>\n","      <th>ip.flags.mf</th>\n","      <th>ip.frag_offset</th>\n","      <th>ip.ttl</th>\n","      <th>ip.proto</th>\n","      <th>ip.checksum</th>\n","      <th>ip.src</th>\n","      <th>ip.dst</th>\n","      <th>ip.len</th>\n","      <th>ip.dsfield</th>\n","      <th>tcp.srcport</th>\n","      <th>tcp.dstport</th>\n","      <th>tcp.seq</th>\n","      <th>tcp.ack</th>\n","      <th>tcp.len</th>\n","      <th>tcp.hdr_len</th>\n","      <th>tcp.flags</th>\n","      <th>ip.id</th>\n","      <th>tcp.flags.fin</th>\n","      <th>tcp.flags.syn</th>\n","      <th>tcp.flags.reset</th>\n","      <th>tcp.flags.push</th>\n","      <th>tcp.flags.ack</th>\n","      <th>tcp.flags.urg</th>\n","      <th>tcp.flags.cwr</th>\n","      <th>tcp.window_size</th>\n","      <th>tcp.checksum</th>\n","      <th>tcp.urgent_pointer</th>\n","      <th>tcp.options.mss_val</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0x0000a005</td>\n","      <td>0x00004000</td>\n","      <td>0.0</td>\n","      <td>TCP</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>64.0</td>\n","      <td>6.0</td>\n","      <td>0x0000312f</td>\n","      <td>10.0.0.15</td>\n","      <td>192.168.2.5</td>\n","      <td>40.0</td>\n","      <td>0x00000000</td>\n","      <td>1809.0</td>\n","      <td>33192.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>24.0</td>\n","      <td>0x00000014</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>28960.0</td>\n","      <td>0x0000eb3b</td>\n","      <td>0.0</td>\n","      <td>1460.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0x00006936</td>\n","      <td>0x00004000</td>\n","      <td>0.0</td>\n","      <td>TCP</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>63.0</td>\n","      <td>6.0</td>\n","      <td>0x0000e304</td>\n","      <td>10.0.0.14</td>\n","      <td>192.168.1.7</td>\n","      <td>52.0</td>\n","      <td>0x00000000</td>\n","      <td>1883.0</td>\n","      <td>33772.0</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>32.0</td>\n","      <td>0x00000010</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>29200.0</td>\n","      <td>0x0000e2fe</td>\n","      <td>0.0</td>\n","      <td>1460.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0x000027e5</td>\n","      <td>0x00000000</td>\n","      <td>0.0</td>\n","      <td>MPEG TS</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>64.0</td>\n","      <td>6.0</td>\n","      <td>0x0000dd72</td>\n","      <td>10.0.0.1</td>\n","      <td>892.168.2.5</td>\n","      <td>52.0</td>\n","      <td>0x00000000</td>\n","      <td>35487.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>32.0</td>\n","      <td>0x00000011</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>29200.0</td>\n","      <td>0x00003f79</td>\n","      <td>0.0</td>\n","      <td>1460.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0x00005067</td>\n","      <td>0x00004000</td>\n","      <td>0.0</td>\n","      <td>TCP</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>64.0</td>\n","      <td>6.0</td>\n","      <td>0x00006a8e</td>\n","      <td>10.0.0.7</td>\n","      <td>192.168.2.5</td>\n","      <td>60.0</td>\n","      <td>0x00000000</td>\n","      <td>1883.0</td>\n","      <td>1883.0</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>20.0</td>\n","      <td>0x00000010</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0x000024f6</td>\n","      <td>0.0</td>\n","      <td>1460.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0x0000b5a4</td>\n","      <td>0x00004000</td>\n","      <td>0.0</td>\n","      <td>TCP</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>64.0</td>\n","      <td>6.0</td>\n","      <td>0x00005dca</td>\n","      <td>192.168.2.5</td>\n","      <td>10.0.0.1</td>\n","      <td>52.0</td>\n","      <td>0x00000000</td>\n","      <td>3700.0</td>\n","      <td>35089.0</td>\n","      <td>50.0</td>\n","      <td>94.0</td>\n","      <td>0.0</td>\n","      <td>24.0</td>\n","      <td>0x00000014</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0x00000735</td>\n","      <td>0.0</td>\n","      <td>1460.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     ip.flags ip.flags.rb  ... tcp.urgent_pointer tcp.options.mss_val\n","0  0x0000a005  0x00004000  ...                0.0              1460.0\n","1  0x00006936  0x00004000  ...                0.0              1460.0\n","2  0x000027e5  0x00000000  ...                0.0              1460.0\n","3  0x00005067  0x00004000  ...                0.0              1460.0\n","4  0x0000b5a4  0x00004000  ...                0.0              1460.0\n","\n","[5 rows x 32 columns]"]},"metadata":{},"execution_count":140}]},{"cell_type":"markdown","metadata":{"id":"Tiru6JkESUtD"},"source":["Now, let's check out the unique values in the columns treated as categorical columns.\n","We expect errors based on the fact that no single model  will have a perfect loss score of 0 and fit perfectly."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kfhZdMxOWHw9","executionInfo":{"status":"ok","timestamp":1632613488214,"user_tz":-60,"elapsed":607,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"c2e27c69-3fb2-486b-f034-1befa3b3ab6e"},"source":["new_samples.columns"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['ip.flags', 'ip.flags.rb', '_ws.col.Protocol', 'ip.flags.df',\n","       'ip.flags.mf', 'ip.frag_offset', 'ip.ttl', 'ip.proto', 'ip.checksum',\n","       'ip.src', 'ip.dst', 'ip.len', 'ip.dsfield', 'tcp.srcport',\n","       'tcp.dstport', 'tcp.seq', 'tcp.ack', 'tcp.len', 'tcp.hdr_len',\n","       'tcp.flags', 'ip.id', 'tcp.flags.fin', 'tcp.flags.syn',\n","       'tcp.flags.reset', 'tcp.flags.push', 'tcp.flags.ack', 'tcp.flags.urg',\n","       'tcp.flags.cwr', 'tcp.window_size', 'tcp.checksum',\n","       'tcp.urgent_pointer', 'tcp.options.mss_val'],\n","      dtype='object')"]},"metadata":{},"execution_count":142}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x2W6gdcbPMTE","executionInfo":{"status":"ok","timestamp":1632612402922,"user_tz":-60,"elapsed":392,"user":{"displayName":"jeffrey otoibhi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11067368294353522262"}},"outputId":"48a1640b-888c-4c2f-d47e-b4d5d475576f"},"source":["for col in cat_columns:\n","  print(col)\n","  print(check[col].unique())\n","  print()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["_ws.col.Protocol\n","['TCP' 'DNS' 'MQTT' 'MPEG TS' 'MDNS' 'MPEG PES' 'ICMP' '0.0' '6.0'\n"," 'TCP32.02' 'UDP' '17.0' '0x00000010' '1.0' '0x00000004' '0x00000000'\n"," '32.0' 'd' '0x00000018' '2' '0x00000019' 'MPEG TS20.0' 'c' 'MPEG TSICMP2'\n"," '0x00004000' '0x00000002' '20.0' 'b' '40.0' '0x00000011' 'TCP2TCP'\n"," 'MQTT1.0MQTT' 'TCPe2' '0x00000012' '9' '44.0' '1' '0x00000014' '265.0'\n"," 'TCPb2' '6.0DNSMPEG PESMQTT' 'MQTTDNS24.0TCP' 'TCP928MQTT'\n"," 'MQTTTCP21460.0' 'TCP0x000000292' 'TCPTCP2.MQTT' 'e' 'f' '0x00000029'\n"," 'MPEG TSMPEG PESMQTT' 'ICMP1.0' 'MPEG TSb2' '24.0' 'TCP0x00000029MQTT'\n"," 'ICMPTCP' 'MQTTTCP' 'TCP0x00000011' 'MDNS0x0000400032.0' 'TCP17.0MQTT'\n"," 'MPEG TSTCP2MDNSTCP' '8' 'MQTTMQTT2' 'DNS0x00004000MQTT' 'TCP1.0' ''\n"," '0x000000000x000000102' 'MPEG TS40.02TCP']\n","\n","ip.flags\n","['0x00000000' '0x00004000' 'MQTT' 'TCP' '0.0'\n"," '0x000040000x000040000x00004000' '32.0' '0x000040001.0.0'\n"," '0x000040000x00004000.0' '1' '0x00004000MQTT0x00004000' '0x00004000.0'\n"," '1460.0' '1.0' 'UDP' '0x0000400060x00004000' '2' '0x00000019'\n"," '0x000040000x00004000' 'ICMP' '0x000040000x00000010'\n"," '0x000000000x000040000x00004000' 'MPEG PES' '0x000040000x000040000'\n"," '0x000040000x00000000' '0x00000000.0' '44.0' '0x00000000a80x00004000'\n"," '265.0' '0x00000014' 'MDNS' 'MPEG TS' '0x00004000b0x00004000' 'DNS'\n"," '0x000000000x00004000' '0x000040000.0' '0x0000400080x00004000']\n","\n","ip.flags.df\n","['0.0' '1.0' '' '0x00000000' '0x00004000' '6.0' '17.0' '.' '1.0.0' 'MQTT'\n"," 'DNS' 'TCP' 'MPEG PES' '0x00000011' '1' '0x00000018' '0x00000029' 'MDNS'\n"," '44.0' '265.0']\n","\n","ip.flags.mf\n","['0.0' '1.0' 'TCP' '0x00004000']\n","\n","ip.flags.rb\n","['0.0' '1.0']\n","\n","ip.frag_offset\n","['0.0' '0x00000000' '1.0' '0.0.']\n","\n","ip.proto\n","['6.0' '17.0' '1.0' '6.01.0' 'MQTT' '' '0x00000002' '6.06.0' '6.0MPEG TS'\n"," 'DNS' '0x00000018' '6.020.0.0' '0x00000010' '6' '20.0' '265.0' '32.0'\n"," '0.0' 'c' '6.0.0' '0x00000014' '0x00000011' 'b' '6.06.04' 'TCP'\n"," '0x00000019' 'ICMP' '0x00000000' '0x00000012' 'MPEG PES' '5' '1460.0'\n"," 'MPEG TS' '3' 'UDP' '40.0' '0x00000004']\n","\n","tcp.flags\n","['0x00000012' '0x00000002' '0x00000010' '1460.0' '0x00000014' '0x00000011'\n"," '0x00000018' '0x00000019' 'DNS' '32.0' '6.0' '0x000000180x00000010.0'\n"," 'ICMP' '0x00000004' '17.0' '4' '44.0' '0x000000100x00000014.0' '20.0'\n"," '265.0' 'MQTT' '0x000000020x000000103' '0x0000000244.0' '40.0' 'MPEG TS'\n"," 'MDNS' '0x000000120.0' 'c' '6' '1.0' '24.0' '0x00000002c0x00000014' 'b'\n"," '0x00000000' '0x000000020x000000104' '1' '0x000000020x000000146'\n"," 'MPEG PES' '0x0000000240.00x00000002.0' '0x00000029' '5'\n"," '0x000000141.0.0' '' '0x000000024' 'd' '0x000000106.00' 'f'\n"," '0x000000020x00000002.0' 'TCP' '0x000000104.0'\n"," '0x000000181.00x00000002.0' '2' '0x000000121.00x00000011'\n"," '0x000000100x00000018' '0x000000190x00000002.0' '0x00000010.2'\n"," '0x000000180x000000112' '0x00000018MPEG TS6' '0x000000020x000000106'\n"," '0x000000110x00000002' '0x000000190x000000195' '0x000000100x000000127'\n"," '0x000000180x00000018.3' '0x0000000244' '0x00000010113'\n"," '0x000000140x00000019.0' '0.0' '0x0000000224' '0x000000100x000000186'\n"," '0x000000120.0.0' '0x0000001480x00000002' '0x00000010.0'\n"," '0x000000180x00000002.0' '0x000000100x00000010.0' '0x000000181.00'\n"," '0x0000001440.0' '0x000000102.0']\n","\n","tcp.flags.ack\n","['0.0' '1.0' '0x00000029' '' '6.0' 'TCP' '2' 'MQTT' '1' '0x00000000' '.'\n"," '0x00004000' '17.0' '0x00000002' '32.0']\n","\n","tcp.flags.cwr\n","['0.0' '1.0' '0x00004000']\n","\n","tcp.flags.fin\n","['0.0' '1.0' 'TCP' '0x00004000' '' '0x00000029' '1']\n","\n","tcp.flags.push\n","['0.0' '1.0' '0x00004000' 'TCP' '0x00000000' '0x00000014' '']\n","\n","tcp.flags.reset\n","['0.0' '1.0' '1' '0x00004000' '' 'TCP' '5' '.' '265.0' '0']\n","\n","tcp.flags.syn\n","['0.0' '1.0' '' 'MDNS' 'TCP' '0x00004000' '265.0' '0.0.0' 'DNS' '1' 'MQTT'\n"," '0x00000011' '6.0']\n","\n","tcp.flags.urg\n","['0.0' '1.0' '0x00004000']\n","\n","tcp.hdr_len\n","['32.0' '40.0' '20.0' '24.0' '0' '32.024.0' '0x00000029' '0x00000014'\n"," '6.0' '265.0' '0x00000002' 'DNS' '1.0' '32.0.0' '0x00000010' '.' 'MQTT'\n"," '' '32.020.0.0' '40.020.0.0' '0x00000012' '7' '0x00000011' '0x00000019'\n"," 'MPEG TS' '0x00004000' '0x00000018' 'ICMP' '17.0' '40.024.0.0' 'TCP' '9'\n"," '32.0632.0' '0x00000000' '32.032.00' 'x' '40.024.0' '6' '40.0.0'\n"," '0x00000004' '1460.0' '20.020.0.0' '0.0' '24.0.0' 'b' '5' '2' '20.0.0'\n"," '24.0c.']\n","\n","tcp.options.mss_val\n","['1460.0' '0x00000014' '0.0' '0x00000011' '265.0' '0x00000018'\n"," '0x00000012' '6' '0x00000010' '6.0' 'MQTT' '17.0' '1460.032.06' '.'\n"," '0x00000004' '0x00000002' '32.0' '' '0x00000019' '1460.01.0.2' '5'\n"," '0x00000000' 'TCP' '1460.0.0' '24.0' '40.0']\n","\n","tcp.urgent_pointer\n","['0.0' '1.0' '0x00004000']\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"sSBNhqYOS6Gx"},"source":["As we can see, these 'Categorical columns' still have some values that were not included in their original and respective unique values. Note, that the 'continous columns' were not observed or taken into consideration here."]}]}